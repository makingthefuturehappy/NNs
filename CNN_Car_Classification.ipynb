{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN_Car_Classification.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/makingthefuturehappy/NNs/blob/main/CNN_Car_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2JFDGIFcpuqF"
      },
      "source": [
        "results:<br>\n",
        "v1.0 'Xception': loss: 0.1225 | accuracy: 0.9696 | submission - 0.93018<br>\n",
        "v1.1. 'Xception' with LR optimization & Batch Optimization: loss: 0.1470 |  accuracy: 0.9693 | submission - 0.93962<br>\n",
        "v2.0 'EfficientNetB4' with LR optimization & Batch Optimization loss: 0.3996 | accuracy: 0.9568 | submission - 0.93018<br>\n",
        "v3.0 'InceptionResNetV2' with LR optimization & Batch Optimization + dataset v3 loss: 0.1252 | accuracy: 0.9926 | submission - 0.96044<br>\n",
        "v3.0 'InceptionResNetV2' with LR optimization & Batch Optimization + dataset v3 + ds v4 loss: 0.0704 - accuracy: 0.9881 | submission - 0.96029\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XUISL8nAGGMG",
        "cellView": "form",
        "outputId": "f73d408a-9a09-4ffc-825e-471f45cc673e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        }
      },
      "source": [
        "#@title Libs setup\n",
        "!pip install gTTS #text to Speach lib for notifications\n",
        "\n",
        "#@title Libs and Drivers setup\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pickle\n",
        "import zipfile\n",
        "import csv\n",
        "import sys\n",
        "import os\n",
        "import shutil\n",
        "import datetime\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler, ModelCheckpoint\n",
        "from tensorflow.keras.callbacks import Callback\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.applications import Xception\n",
        "from tensorflow.keras.applications import EfficientNetB4\n",
        "\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "\n",
        "import PIL\n",
        "from PIL import ImageOps, ImageFilter\n",
        "#увеличим дефолтный размер графиков\n",
        "from pylab import rcParams\n",
        "rcParams['figure.figsize'] = 10, 5\n",
        "#графики в svg выглядят более четкими\n",
        "%config InlineBackend.figure_format = 'svg' \n",
        "%matplotlib inline\n",
        "%config IPCompleter.greedy=True  # более лучший автокомплит в блокноте\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "file_path = 'https://drive.google.com/drive/folders/1Sc9qSYEE4Mf2z1MVccxwcUCW3MqTdnwa'\n",
        "\n",
        "def Notification(text = 'task is done'):\n",
        "  tts = gTTS(text)\n",
        "  print(text)\n",
        "  tts.save('notification.wav')\n",
        "  sound_file = 'notification.wav'\n",
        "  return Audio(sound_file, autoplay=True)\n",
        "\n",
        "# from google.colab import output\n",
        "# def Notification():\n",
        "#   output.eval_js('new Audio(\"https://soundslibmp3.ru/sounds/1599371653_intro-35.mp3\").play()')"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gTTS in /usr/local/lib/python3.6/dist-packages (2.1.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gTTS) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from gTTS) (7.1.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from gTTS) (4.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from gTTS) (2.23.0)\n",
            "Requirement already satisfied: gtts-token>=1.1.3 in /usr/local/lib/python3.6/dist-packages (from gTTS) (1.1.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->gTTS) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->gTTS) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->gTTS) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->gTTS) (3.0.4)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJUPMjkiG3lh",
        "cellView": "form",
        "outputId": "532b587c-dd0b-408c-c7cf-44520440cf3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        }
      },
      "source": [
        "#@title system check\n",
        "Notification()\n",
        "print('Python       :', sys.version.split('\\n')[0])\n",
        "print('Numpy        :', np.__version__)\n",
        "print('Tensorflow   :', tf.__version__)\n",
        "print('Keras        :', tf.keras.__version__)\n",
        "tf.test.gpu_device_name() ## Проверяем что у нас работает GPU\n",
        "!nvidia-smi -L ##GPU count and name"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "task is done\n",
            "Python       : 3.6.9 (default, Jul 17 2020, 12:50:27) \n",
            "Numpy        : 1.18.5\n",
            "Tensorflow   : 2.3.0\n",
            "Keras        : 2.4.0\n",
            "NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mwiYkC3JyDX",
        "cellView": "form"
      },
      "source": [
        "#@title Variables Setup\n",
        "EPOCHS               = 7  # эпох на обучение\n",
        "BATCH_SIZE           = 256 # уменьшаем batch если сеть большая, иначе не влезет в память на GPU\n",
        "LR                   = 1e-4\n",
        "VAL_SPLIT            = 0.15 # сколько данных выделяем на тест = 15%\n",
        "\n",
        "CLASS_NUM            = 10  # количество классов в нашей задаче\n",
        "IMG_SIZE             = 224 # какого размера подаем изображения в сеть\n",
        "IMG_CHANNELS         = 3   # у RGB 3 канала\n",
        "input_shape          = (IMG_SIZE, IMG_SIZE, IMG_CHANNELS)\n",
        "\n",
        "PATH = '/content/'\n",
        "PATH_GDRIVE = '/content/drive/My Drive/Colab Notebooks/car classification/data/'\n",
        "\n",
        "# Устаналиваем конкретное значение random seed для воспроизводимости\n",
        "# os.makedirs(PATH,exist_ok=False)\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)  \n",
        "PYTHONHASHSEED = 0"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wAxRffMIVEp8",
        "cellView": "form"
      },
      "source": [
        "# @title Dataset Setup\n",
        "print('unpacking the pics...')\n",
        "\n",
        "# train DataSet Setup\n",
        "print('unpacking train dataset...')\n",
        "with zipfile.ZipFile(PATH_GDRIVE + 'train/' + 'aug_train_v3.zip',\"r\") as z:\n",
        "  z.extractall(PATH + 'train')\n",
        "\n",
        "## test DataSet Setup\n",
        "print('unpacking test dataset...')\n",
        "with zipfile.ZipFile(PATH_GDRIVE + 'test/test_upload.zip',\"r\") as z:\n",
        "        z.extractall(PATH)\n",
        "\n",
        "## copy files from G-Drive to Colab env.\n",
        "%cp -av '/content/drive/My Drive/Colab Notebooks/car classification/data/train.csv' '/content'\n",
        "%cp -av '/content/drive/My Drive/Colab Notebooks/car classification/data/sample-submission.csv' '/content'\n",
        "\n",
        "print('cleaning folders...')\n",
        "shutil.rmtree(PATH + 'train/__MACOSX')\n",
        "shutil.rmtree(PATH + '__MACOSX')\n",
        "# shutil.rmtree(PATH + 'sample_data')\n",
        "print('data uploaded...')\n",
        "Notification()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXVw8GCFXTGG",
        "cellView": "form"
      },
      "source": [
        "# @title Delete all internal directories (muted)\n",
        "# import shutil\n",
        "\n",
        "# for dir_name in range (0,9,1):\n",
        "#   dir_name = str(dir_name)\n",
        "#   try:\n",
        "#     shutil.rmtree('/content/' + str(dir_name))\n",
        "#   except: print(dir_name, 'dir does not exist')\n",
        "\n",
        "# try:\n",
        "#   shutil.rmtree('/content/' + str('__MACOSX'))\n",
        "# except: print(dir_name, 'dir does not exist')\n",
        "\n",
        "# try:\n",
        "#   shutil.rmtree(PATH + 'train/')\n",
        "#   shutil.rmtree(PATH + 'test_upload/')\n",
        "# except: print('deleting all files error')\n",
        "\n",
        "# print('allDone')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82lojgMBZxap",
        "cellView": "form"
      },
      "source": [
        "#@title Random pics examples (muted)\n",
        "# print('random pics examples')\n",
        "\n",
        "# train_df = pd.read_csv(PATH+\"train.csv\")\n",
        "# sample_submission = pd.read_csv(PATH+\"sample-submission.csv\")\n",
        "# print('dataset examples\\n',\n",
        "#       train_df.head(),\n",
        "#       '\\n\\ncategories destribution\\n',\n",
        "#       train_df.Category.value_counts())\n",
        "\n",
        "# plt.figure(figsize=(12,8))\n",
        "\n",
        "# random_image = train_df.sample(n=9)\n",
        "# random_image_paths = random_image['Id'].values\n",
        "# random_image_cat = random_image['Category'].values\n",
        "\n",
        "# for index, path in enumerate(random_image_paths):\n",
        "#     im = PIL.Image.open(PATH+f'train/{random_image_cat[index]}/{path}')\n",
        "#     plt.subplot(3,3, index+1)\n",
        "#     plt.imshow(im)\n",
        "#     plt.title('Class: '+str(random_image_cat[index]))\n",
        "#     plt.axis('off')\n",
        "# plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLVi-F5IWjKQ",
        "cellView": "form"
      },
      "source": [
        "#@title Pics Generator setup\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1. / 255,\n",
        "    validation_split=VAL_SPLIT, # set validation split\n",
        "    ##pics augumentation\n",
        "    # horizontal_flip=False\n",
        "    # rotation_range = 5,\n",
        "    # width_shift_range=0.1,\n",
        "    # height_shift_range=0.1\n",
        "    )\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    PATH + 'train/',      # директория где расположены папки с картинками \n",
        "    target_size=(IMG_SIZE, IMG_SIZE),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical',\n",
        "    shuffle=True, seed=RANDOM_SEED,\n",
        "    subset='training') # set as training data\n",
        "\n",
        "test_generator = train_datagen.flow_from_directory(\n",
        "    PATH + 'train/',\n",
        "    target_size=(IMG_SIZE, IMG_SIZE),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical',\n",
        "    shuffle=True, seed=RANDOM_SEED,\n",
        "    subset='validation') # set as validation data\n",
        "\n",
        "sample_submission = pd.read_csv(PATH+\"sample-submission.csv\")\n",
        "test_sub_generator = test_datagen.flow_from_dataframe( \n",
        "    dataframe=sample_submission,\n",
        "    directory=PATH+'test_upload/',\n",
        "    x_col=\"Id\",\n",
        "    y_col=None,\n",
        "    shuffle=False,\n",
        "    class_mode=None,\n",
        "    seed=RANDOM_SEED,\n",
        "    target_size=(IMG_SIZE, IMG_SIZE),\n",
        "    batch_size=BATCH_SIZE,)\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1. / 255)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dn85jMN9rToJ",
        "cellView": "both",
        "outputId": "7fb5c460-9836-4dcb-e9eb-a6efed085141",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "#@title Tensorboard load\n",
        "%load_ext tensorboard\n",
        "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "!rm -rf ./logs/\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir,\n",
        "                                                      histogram_freq=1)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "io8kBhFM-EUa",
        "cellView": "form"
      },
      "source": [
        "#@title plot_result function\n",
        "def plot_result(history):\n",
        "    acc = history.history['accuracy']\n",
        "    val_acc = history.history['val_accuracy']\n",
        "    loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "\n",
        "    epochs = range(len(acc))\n",
        "\n",
        "    plt.plot(epochs, acc, 'b', label='Training acc')\n",
        "    plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
        "    plt.title('Training and validation accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.figure()\n",
        "\n",
        "    plt.plot(epochs, loss, 'b', label='Training loss')\n",
        "    plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
        "    plt.title('Training and validation loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.show()  \n",
        "    return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Kjb9wwgh83v"
      },
      "source": [
        "# Xception tests"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_LBnB9qYeKL",
        "cellView": "both"
      },
      "source": [
        "#@title NN_v1.0 Xception (setup)\n",
        "model_name = 'v1_0.hdf5'\n",
        "base_model = Xception(weights='imagenet', include_top=False, input_shape = input_shape)\n",
        "# New head setup\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "# let's add a fully-connected layer\n",
        "x = Dense(256, activation='relu')(x)\n",
        "x = Dropout(0.25)(x)\n",
        "# and a logistic layer -- let's say we have 10 classes\n",
        "predictions = Dense(CLASS_NUM, activation='softmax')(x)\n",
        "\n",
        "# this is the model we will train\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "model.compile(loss=\"categorical_crossentropy\", \n",
        "              optimizer=optimizers.Adam(lr=LR), \n",
        "              metrics=[\"accuracy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKK_-IDhY5ES",
        "cellView": "form"
      },
      "source": [
        "#@title NN_v1.0 Xception (training)\n",
        "checkpoint = ModelCheckpoint(model_name ,\n",
        "                             monitor = ['val_accuracy'],\n",
        "                             verbose = 1, \n",
        "                             mode = 'max')\n",
        "callbacks_list = [checkpoint]\n",
        "\n",
        "history = model.fit(\n",
        "        train_generator,\n",
        "        steps_per_epoch = len(train_generator),\n",
        "        validation_data = test_generator, \n",
        "        validation_steps = len(test_generator),\n",
        "        epochs = EPOCHS,\n",
        "        callbacks=[tensorboard_callback]\n",
        "        )\n",
        "\n",
        "model.save(PATH_GDRIVE + \"models/\" + model_name)\n",
        "scores = model.evaluate_generator(test_generator, steps=len(test_generator), verbose=1)\n",
        "print(\"\\nAccuracy: %.2f%%\" % (scores[1]*100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1PZbgI_Z83X",
        "cellView": "form"
      },
      "source": [
        "#@title NN_v1.1 Xception with LR optimization & Batch Optimization (setup)\n",
        "%reload_ext tensorboard\n",
        "model_name = 'v1_1.hdf5'\n",
        "!rm -rf ./logs/\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir,\n",
        "                                                      histogram_freq=1)\n",
        "\n",
        "base_model = Xception(weights='imagenet', include_top=False, input_shape = input_shape)\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dropout(0.3)(x)\n",
        "x = Dense(100, activation='relu', kernel_regularizer = 'l2')(x)\n",
        "x = BatchNormalization()(x)\n",
        "predictions = Dense(CLASS_NUM, activation='softmax')(x)\n",
        "\n",
        "# this is the model we will train\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "model.compile(loss=\"categorical_crossentropy\",\n",
        "              optimizer=optimizers.Adam(learning_rate = tf.keras.optimizers.schedules.ExponentialDecay(0.0005, \n",
        "                                                                                            decay_steps = 100, \n",
        "                                                                                            decay_rate = 0.9)),\n",
        "                                        metrics=[\"accuracy\"])\n",
        "\n",
        "checkpoint = ModelCheckpoint(model_name , \n",
        "                             monitor = ['val_accuracy'] , \n",
        "                             verbose = 1  , mode = 'max')\n",
        "callbacks_list = [checkpoint]\n",
        "\n",
        "tf.keras.callbacks.ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    factor=0.1, # new_lr = lr * factor. \n",
        "    patience=2,\n",
        "    verbose=1,\n",
        "    mode='auto',\n",
        "    min_delta=0.0001, cooldown=1, min_lr=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v7Z2hTLX-1hu",
        "cellView": "form"
      },
      "source": [
        "#@title NN_v1.1 Xception with LR optimization & Batch Optimization (training)\n",
        "history = model.fit(\n",
        "        train_generator,\n",
        "        steps_per_epoch = len(train_generator),\n",
        "        validation_data = test_generator, \n",
        "        validation_steps = len(test_generator),\n",
        "        epochs = EPOCHS,\n",
        "        callbacks = callbacks_list)\n",
        "\n",
        "model.save(PATH_GDRIVE + 'models/' + model_name)\n",
        "scores = model.evaluate_generator(test_generator, steps=len(test_generator), verbose=1)\n",
        "print(\"\\nAccuracy: %.2f%%\" % (scores[1]*100))\n",
        "%tensorboard --logdir logs/fit"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kY0xn1gRFzuh"
      },
      "source": [
        "# EfficientNetB4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOWqJtnHiMeQ"
      },
      "source": [
        "## basic experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UlVBIHfwF6mU"
      },
      "source": [
        "### ReduceLROnPlateau"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUwkoWL5GFnv",
        "cellView": "form"
      },
      "source": [
        "#@title NN_v3.0 'InceptionResNetV2' with LR optimization & Batch Optimization (setup)\n",
        "from tensorflow.keras.applications import InceptionResNetV2\n",
        "\n",
        "model_name = 'v3_0_ds4.hdf5'\n",
        "base_model = InceptionResNetV2(input_shape=input_shape,\n",
        "                          include_top=False,\n",
        "                          weights=\"imagenet\")\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dropout(0.3)(x)\n",
        "x = Dense(100, activation='relu', kernel_regularizer = 'l2')(x)\n",
        "x = BatchNormalization()(x)\n",
        "predictions = Dense(CLASS_NUM, activation='softmax')(x)\n",
        "\n",
        "# this is the model we will train\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "model.compile(loss=\"categorical_crossentropy\",\n",
        "              optimizer=optimizers.Adam(learning_rate = tf.keras.optimizers.schedules.ExponentialDecay(0.0005, \n",
        "                                                                                            decay_steps = 100, \n",
        "                                                                                            decay_rate = 0.9)),\n",
        "                                        metrics=[\"accuracy\"])\n",
        "\n",
        "checkpoint = ModelCheckpoint(model_name , \n",
        "                             monitor = ['val_accuracy'] , \n",
        "                             verbose = 1  , mode = 'max')\n",
        "callbacks_list = [checkpoint]\n",
        "\n",
        "tf.keras.callbacks.ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    factor=0.1, # new_lr = lr * factor. \n",
        "    patience=2,\n",
        "    verbose=1,\n",
        "    mode='auto',\n",
        "    min_delta=0.0001, cooldown=1, min_lr=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lt6qEGq1HI6g",
        "cellView": "form"
      },
      "source": [
        "#@title NN_v3.0 'InceptionResNetV2' with LR optimization & Batch Optimization (training)\n",
        "\n",
        "history = model.fit(\n",
        "        train_generator,\n",
        "        steps_per_epoch = len(train_generator),\n",
        "        validation_data = test_generator, \n",
        "        validation_steps = len(test_generator),\n",
        "        epochs = EPOCHS,\n",
        "        callbacks = callbacks_list)\n",
        "\n",
        "model.save(PATH_GDRIVE + 'models/' + model_name)\n",
        "\n",
        "scores = model.evaluate_generator(test_generator, steps=len(test_generator), verbose=1)\n",
        "print(\"\\nAccuracy: %.2f%%\" % (scores[1]*100))\n",
        "plot_result(history)\n",
        "Notification()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTNw2u_cGsTN"
      },
      "source": [
        "### without ReduceLROnPlateau"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "qSNZb8GMG4Pw"
      },
      "source": [
        "#@title NN_v3.1 'InceptionResNetV2' with No LR optimization & Batch Optimization (setup)\n",
        "from tensorflow.keras.applications import InceptionResNetV2\n",
        "\n",
        "model_name = 'v3_1_ds2.hdf5'\n",
        "base_model = InceptionResNetV2(input_shape=input_shape,\n",
        "                          include_top=False,\n",
        "                          weights=\"imagenet\")\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dropout(0.3)(x)\n",
        "x = Dense(100, activation='relu', kernel_regularizer = 'l2')(x)\n",
        "x = BatchNormalization()(x)\n",
        "predictions = Dense(CLASS_NUM, activation='softmax')(x)\n",
        "\n",
        "# this is the model we will train\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "model.compile(loss=\"categorical_crossentropy\",\n",
        "              optimizer=optimizers.Adam(learning_rate = tf.keras.optimizers.schedules.ExponentialDecay(0.0005, \n",
        "                                                                                            decay_steps = 100, \n",
        "                                                                                            decay_rate = 0.9)),\n",
        "                                        metrics=[\"accuracy\"])\n",
        "\n",
        "checkpoint = ModelCheckpoint(model_name , \n",
        "                             monitor = ['val_accuracy'] , \n",
        "                             verbose = 1  , mode = 'max')\n",
        "callbacks_list = [checkpoint]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "AUShqOqFG50E"
      },
      "source": [
        "#@title NN_v3.1 'InceptionResNetV2' with No LR optimization & Batch Optimization (training)\n",
        "\n",
        "history = model.fit(\n",
        "        train_generator,\n",
        "        steps_per_epoch = len(train_generator),\n",
        "        validation_data = test_generator, \n",
        "        validation_steps = len(test_generator),\n",
        "        epochs = EPOCHS,\n",
        "        callbacks = callbacks_list)\n",
        "\n",
        "model.save(PATH_GDRIVE + 'models/' + model_name)\n",
        "\n",
        "scores = model.evaluate_generator(test_generator, steps=len(test_generator), verbose=1)\n",
        "print(\"\\nAccuracy: %.2f%%\" % (scores[1]*100))\n",
        "plot_result(history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpQTRUTV1UWl"
      },
      "source": [
        "## Fine-Tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPx2Nf69ZxMi"
      },
      "source": [
        "**the idea is:**<br>\n",
        "- to find the  optimal hyperparamets by using TensorBoard (TB)<br>\n",
        "- train the net<br>\n",
        "- add more augmented pics to the categories with the highest confusion according to the confusion matrix<br>\n",
        "- re-train the net<br>\n",
        "- apply the fine-tuning<br><br>\n",
        "<b>TO DO:</b><br>\n",
        "<input type=\"checkbox\">prepare the middle-sized augmented dataset (train_ds_v5))<br>\n",
        "<input type=\"checkbox\">setup TB, API HParams, ConfMatrix<br>\n",
        "<input type=\"checkbox\">pre-train CNN (1 EPOCH) to find the optimal param <br>\n",
        "<input type=\"checkbox\">full train CNN with the optimal param <br>\n",
        "<input type=\"checkbox\">add more augmented pics (train_ds_v6)<br> \n",
        "<input type=\"checkbox\">full train<br>\n",
        "<input type=\"checkbox\">fine tuning<br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_kePxVftUQe",
        "cellView": "both"
      },
      "source": [
        "#@title NN_v3.2 'InceptionResNetV2' with LR & Batch Optimization + FineTuning (setup)\n",
        "\n",
        "from tensorflow.keras.applications import InceptionResNetV2\n",
        "\n",
        "model_name = 'v3_2_ds4.hdf5'\n",
        "base_model = InceptionResNetV2(input_shape=input_shape,\n",
        "                          include_top=False,\n",
        "                          weights=\"imagenet\")\n",
        "# Freeze the base_model\n",
        "base_model.trainable = False\n",
        "\n",
        "x = base_model.output\n",
        "x = BatchNormalization()(x, training=False)\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dropout(0.3)(x)\n",
        "x = Dense(100, activation='relu', kernel_regularizer = 'l2')(x)\n",
        "x = BatchNormalization()(x)\n",
        "predictions = Dense(CLASS_NUM, activation='softmax')(x)\n",
        "\n",
        "learning_rate = \n",
        "\n",
        "optimizer = \n",
        "\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "model.compile(loss=\"categorical_crossentropy\",\n",
        "              optimizer=optimizers.Adam(learning_rate = tf.keras.optimizers.schedules.ExponentialDecay(0.0005, \n",
        "                                                                                            decay_steps = 100, \n",
        "                                                                                            decay_rate = 0.9)),\n",
        "                                        metrics=[\"accuracy\"])\n",
        "\n",
        "checkpoint = ModelCheckpoint(model_name , \n",
        "                             monitor = ['val_accuracy'] , \n",
        "                             verbose = 1  , mode = 'max')\n",
        "callbacks_list = [checkpoint]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "7_N-yTpWw0uS"
      },
      "source": [
        "#@title NN_v3.2 'InceptionResNetV2' with LR & Batch Optimization + FineTuning (training)\n",
        "history = model.fit(\n",
        "        train_generator,\n",
        "        steps_per_epoch = len(train_generator),\n",
        "        validation_data = test_generator, \n",
        "        validation_steps = len(test_generator),\n",
        "        epochs = EPOCHS,\n",
        "        callbacks = callbacks_list)\n",
        "\n",
        "model.save(PATH_GDRIVE + 'models/' + model_name)\n",
        "\n",
        "scores = model.evaluate_generator(test_generator, steps=len(test_generator), verbose=1)\n",
        "print(\"\\nAccuracy: %.2f%%\" % (scores[1]*100))\n",
        "plot_result(history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BuFu1qa8xWKS",
        "cellView": "form"
      },
      "source": [
        "#@title NN_v3.2 'InceptionResNetV2' with LR & Batch Optimization + FineTuning (fine-tuning)\n",
        "model_name = 'v3_2_ds4_finetuned.hdf5'\n",
        "base_model.trainable = True\n",
        "model.compile(loss=\"categorical_crossentropy\",\n",
        "              optimizer=optimizers.Adam(1e-5))\n",
        "\n",
        "checkpoint = ModelCheckpoint(model_name , \n",
        "                             monitor = ['val_accuracy'] , \n",
        "                             verbose = 1  , mode = 'max')\n",
        "callbacks_list = [checkpoint]\n",
        "\n",
        "history = model.fit(\n",
        "        train_generator,\n",
        "        steps_per_epoch = len(train_generator),\n",
        "        validation_data = test_generator, \n",
        "        validation_steps = len(test_generator),\n",
        "        epochs = EPOCHS,\n",
        "        callbacks = callbacks_list)\n",
        "\n",
        "model.save(PATH_GDRIVE + 'models/' + model_name)\n",
        "\n",
        "scores = model.evaluate_generator(test_generator, steps=len(test_generator), verbose=1)\n",
        "print(\"\\nAccuracy: %.2f%%\" % (scores[1]*100))\n",
        "plot_result(history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vX8cDwP6CoWu"
      },
      "source": [
        "## submission generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5lMVvQpj5f2R"
      },
      "source": [
        "model.load_weights(PATH_GDRIVE + 'models/'+\n",
        "                  #  'v1_0.hdf5',\n",
        "                  #  'v1_1.hdf5',\n",
        "                  #  'v2_0.hdf5',\n",
        "                  #  'v3_0.hdf5',\n",
        "                  #  'v3_1.hdf5',\n",
        "                  #  'v3_0_ds3.hdf5',\n",
        "                  #  'v3_0_ds4.hdf5',\n",
        "                   'v3_2_ds4_finetuned.hdf5'\n",
        "                   )\n",
        "scores = model.evaluate(test_generator, steps=len(test_generator), verbose=1)\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
        "\n",
        "test_sub_generator.reset()\n",
        "predictions = model.predict(test_sub_generator, steps=len(test_sub_generator), verbose=1) \n",
        "predictions = np.argmax(predictions, axis=-1) #multiple categories\n",
        "label_map = (train_generator.class_indices)\n",
        "label_map = dict((v,k) for k,v in label_map.items()) #flip k,v\n",
        "predictions = [label_map[k] for k in predictions]\n",
        "\n",
        "filenames_with_dir=test_sub_generator.filenames\n",
        "submission = pd.DataFrame({'Id':filenames_with_dir, 'Category':predictions}, columns=['Id', 'Category'])\n",
        "submission['Id'] = submission['Id'].replace('test_upload/','')\n",
        "submission.to_csv(PATH_GDRIVE + 'submission.csv', index=False)\n",
        "print('Submit saved.')\n",
        "Notification()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvXRiQ_h3voF"
      },
      "source": [
        "import os, signal\n",
        "os.kill(os.getpid(),signal.SIGKILL)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}